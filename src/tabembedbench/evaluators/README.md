# Evaluators Module

This module contains evaluation algorithms that assess the quality of tabular embeddings across different machine learning tasks. All evaluators implement a common interface defined by the `AbstractEvaluator` base class.

## Overview

The evaluators module provides standardized evaluation methods for:
- **Classification tasks**: Using k-nearest neighbors (KNN) classifiers
- **Regression tasks**: Using k-nearest neighbors (KNN) regressors  
- **Outlier detection**: Using Local Outlier Factor (LOF) and Isolation Forest algorithms

All evaluators work with embeddings generated by the embedding models and provide consistent interfaces for training, prediction, and parameter management.

## Base Class: AbstractEvaluator

The `AbstractEvaluator` class defines the common interface that all evaluation algorithms must implement.

### Key Methods

#### Abstract Methods (must be implemented by subclasses):
- `get_prediction(embeddings, y=None, train=True)`: Generates predictions from embeddings
- `reset_evaluator()`: Resets the evaluator state between datasets
- `get_parameters()`: Returns the evaluator's configuration parameters

#### Concrete Methods:
- `get_task()`: Returns the task type (classification, regression, outlier_detection)

### Constructor Parameters:
- `name`: Identifier for the evaluator instance
- `task_type`: Type of task ("classification", "regression", "outlier_detection")

## Available Evaluators

### 1. KNNClassifierEvaluator (`classifier.py`)

Implements k-nearest neighbors classification for evaluating embeddings on classification tasks.

**Key Features:**
- Configurable number of neighbors
- Multiple distance metrics (euclidean, cosine, etc.)
- Weighted and uniform neighbor voting
- ROC-AUC scoring for binary and multiclass problems

**Parameters:**
- `num_neighbors`: Number of nearest neighbors to consider
- `weights`: Weighting scheme ("uniform" or "distance")
- `metric`: Distance metric for neighbor search

**Usage in Benchmarks:**
- Used in TabArena benchmark for classification datasets
- Evaluates embedding quality through downstream classification performance

### 2. KNNRegressorEvaluator (`regression.py`)

Implements k-nearest neighbors regression for evaluating embeddings on regression tasks.

**Key Features:**
- Configurable number of neighbors
- Multiple distance metrics
- Weighted and uniform neighbor averaging
- Mean squared error (MSE) scoring

**Parameters:**
- `num_neighbors`: Number of nearest neighbors to consider
- `weights`: Weighting scheme ("uniform" or "distance")
- `metric`: Distance metric for neighbor search

**Usage in Benchmarks:**
- Used in TabArena benchmark for regression datasets
- Evaluates embedding quality through downstream regression performance

### 3. LocalOutlierFactorEvaluator (`outlier.py`)

Implements Local Outlier Factor algorithm for outlier detection using embeddings.

**Key Features:**
- Density-based outlier detection
- Configurable neighborhood size
- Multiple distance metrics
- AUROC scoring for outlier detection performance

**Parameters:**
- `model_params`: Dictionary containing LOF parameters
  - `n_neighbors`: Number of neighbors for density estimation
  - `metric`: Distance metric for neighbor search

**Usage in Benchmarks:**
- Primary evaluator for ADBench outlier detection benchmark
- Assesses how well embeddings preserve outlier patterns

### 4. IsolationForestEvaluator (`outlier.py`)

Implements Isolation Forest algorithm for outlier detection using embeddings.

**Key Features:**
- Tree-based outlier detection
- Configurable ensemble size
- Fast training and prediction
- AUROC scoring for outlier detection performance

**Parameters:**
- `model_params`: Dictionary containing Isolation Forest parameters
  - `n_estimators`: Number of trees in the ensemble

**Usage in Benchmarks:**
- Alternative outlier detection method in benchmarks
- Provides comparison with LOF-based evaluation

## Integration with Benchmarking Framework

### Workflow Integration
1. **Embedding Generation**: Embedding models generate vector representations
2. **Evaluator Selection**: Appropriate evaluators are chosen based on task type
3. **Training/Prediction**: Evaluators use embeddings to make predictions
4. **Performance Measurement**: Metrics are computed and collected
5. **State Reset**: Evaluators are reset between different datasets

### Parameter Sweeps
The benchmarking framework automatically creates evaluator instances with different parameter combinations:
- **Neighbor counts**: Typically 5-50 neighbors for KNN-based evaluators
- **Distance metrics**: Euclidean and cosine distance
- **Weighting schemes**: Uniform and distance-based weighting
- **Ensemble sizes**: Various tree counts for Isolation Forest

### Performance Metrics
- **Classification**: ROC-AUC score
- **Regression**: Mean Squared Error (MSE)
- **Outlier Detection**: Area Under ROC Curve (AUROC)

## Usage Examples

### Basic Usage
```python
from tabembedbench.evaluators import KNNClassifierEvaluator

# Create evaluator
evaluator = KNNClassifierEvaluator(
    num_neighbors=5,
    weights="uniform", 
    metric="euclidean"
)

# Use in benchmark
predictions = evaluator.get_prediction(embeddings, y_train, train=True)
test_predictions = evaluator.get_prediction(test_embeddings, train=False)
```

### Integration with Benchmarks
```python
from tabembedbench.benchmark import run_benchmark
from tabembedbench.evaluators import KNNClassifierEvaluator, LocalOutlierFactorEvaluator

evaluators = [
    KNNClassifierEvaluator(num_neighbors=5, metric="euclidean"),
    LocalOutlierFactorEvaluator(model_params={"n_neighbors": 10})
]

results = run_benchmark(
    embedding_models=embedding_models,
    evaluator_algorithms=evaluators
)
```

## Adding New Evaluators

To add a new evaluation algorithm:

1. **Inherit from AbstractEvaluator**:
   ```python
   class MyEvaluator(AbstractEvaluator):
       def __init__(self, ...):
           super().__init__(name="MyEvaluator", task_type="classification")
   ```

2. **Implement required methods**:
   - `get_prediction()`: Core prediction logic
   - `reset_evaluator()`: State cleanup
   - `get_parameters()`: Parameter reporting

3. **Follow naming conventions**: Use descriptive names that indicate the algorithm and task type

4. **Test integration**: Ensure compatibility with the benchmarking framework

## Dependencies

The evaluators module relies on:
- **scikit-learn**: For KNN, LOF, and Isolation Forest implementations
- **numpy**: For numerical operations and array handling
- **abc**: For abstract base class functionality

## Notes

- All evaluators are designed to work with numpy arrays of embeddings
- State management is crucial - always call `reset_evaluator()` between datasets
- Parameter dictionaries allow flexible configuration of underlying algorithms
- The module supports both supervised (classification/regression) and unsupervised (outlier detection) evaluation tasks
