# Evaluators Module

This module contains evaluation algorithms that assess the quality of tabular embeddings across different machine learning tasks. All evaluators implement a common interface defined by the `AbstractEvaluator` base class.

## Overview

The evaluators module provides standardized evaluation methods for:
- **Classification tasks**: Using k-nearest neighbors (KNN) classifiers
- **Regression tasks**: Using k-nearest neighbors (KNN) regressors  
- **Outlier detection**: Using Local Outlier Factor (LOF) and Isolation Forest algorithms

All evaluators work with embeddings generated by the embedding models and provide consistent interfaces for training, prediction, and parameter management.

## Base Class: AbstractEvaluator

The `AbstractEvaluator` class defines the common interface that all evaluation algorithms must implement.

### Key Methods

#### Abstract Methods (must be implemented by subclasses):
- `get_prediction(embeddings, y=None, train=True)`: Generates predictions from embeddings
- `reset_evaluator()`: Resets the evaluator state between datasets
- `get_parameters()`: Returns the evaluator's configuration parameters

#### Concrete Methods:
- `get_task()`: Returns the task type (classification, regression, outlier_detection)

### Constructor Parameters:
- `name`: Identifier for the evaluator instance
- `task_type`: Type of task ("classification", "regression", "outlier_detection")

## Available Evaluators

### 1. KNNClassifierEvaluator (`knn_classifier.py`)

Implements k-nearest neighbors classification for evaluating embeddings on classification tasks.

**Key Features:**
- Configurable number of neighbors
- Multiple distance metrics (euclidean, cosine, etc.)
- Weighted and uniform neighbor voting
- ROC-AUC scoring for binary and multiclass problems

**Parameters:**
- `num_neighbors`: Number of nearest neighbors to consider
- `weights`: Weighting scheme ("uniform" or "distance")
- `metric`: Distance metric for neighbor search

**Usage in Benchmarks:**
- Used in TabArena benchmark for classification datasets
- Evaluates embedding quality through downstream classification performance

### 2. KNNRegressorEvaluator (`knn_regressor.py`)

Implements k-nearest neighbors regression for evaluating embeddings on regression tasks.

**Key Features:**
- Configurable number of neighbors
- Multiple distance metrics
- Weighted and uniform neighbor averaging
- Mean squared error (MSE) scoring

**Parameters:**
- `num_neighbors`: Number of nearest neighbors to consider
- `weights`: Weighting scheme ("uniform" or "distance")
- `metric`: Distance metric for neighbor search

**Usage in Benchmarks:**
- Used in TabArena benchmark for regression datasets
- Evaluates embedding quality through downstream regression performance

### 3. MLPClassifierEvaluator (`mlp_classifier.py`)

Implements Multi-Layer Perceptron (MLP) neural network classifier with automatic hyperparameter optimization using Optuna.

**Key Features:**
- Hyperparameter optimization via Optuna
- Cross-validation for robust model selection
- Sklearn-based MLP implementation with custom wrapper
- Early stopping and validation monitoring
- Standard scaling of input features

**Inherits From:**
- `AbstractHPOEvaluator`: Provides Optuna-based hyperparameter optimization framework

**Parameters:**
- `n_trials`: Number of Optuna optimization trials (default: 10)
- `cv_folds`: Number of cross-validation folds (default: 5)
- `random_state`: Random seed for reproducibility (default: 42)
- `verbose`: Whether to print optimization progress (default: False)

**Optimized Hyperparameters:**
- `n_layers`: Number of hidden layers (1-5)
- `hidden_layer_sizes`: Tuple of hidden layer dimensions (32-512 per layer)
- `alpha`: L2 regularization strength (1e-5 to 1e-1)
- `learning_rate_init`: Initial learning rate (1e-4 to 1e-2)
- `batch_size`: Training batch size (16, 32, 64, 128)
- `max_iter`: Maximum training iterations (50-200)
- `activation`: Activation function ('relu', 'tanh', 'logistic')

**Usage in Benchmarks:**
- Provides neural network-based evaluation for classification tasks
- Alternative to KNN-based classification evaluation
- Automatic hyperparameter tuning for optimal performance

### 4. MLPRegressorEvaluator (`mlp_regressor.py`)

Implements Multi-Layer Perceptron (MLP) neural network regressor with automatic hyperparameter optimization using Optuna.

**Key Features:**
- Hyperparameter optimization via Optuna
- Cross-validation for robust model selection
- Sklearn-based MLP implementation with custom wrapper
- Early stopping and validation monitoring
- Standard scaling of input features

**Inherits From:**
- `AbstractHPOEvaluator`: Provides Optuna-based hyperparameter optimization framework

**Parameters:**
- `n_trials`: Number of Optuna optimization trials (default: 10)
- `cv_folds`: Number of cross-validation folds (default: 5)
- `random_state`: Random seed for reproducibility (default: 42)
- `verbose`: Whether to print optimization progress (default: False)

**Optimized Hyperparameters:**
- `n_layers`: Number of hidden layers (1-3)
- `hidden_layer_sizes`: Tuple of hidden layer dimensions (32-512 per layer)
- `alpha`: L2 regularization strength (1e-5 to 1e-1)
- `learning_rate_init`: Initial learning rate (1e-4 to 1e-2)
- `batch_size`: Training batch size (16, 32, 64, 128, 256)
- `max_iter`: Maximum training iterations (50-200)
- `activation`: Activation function ('relu', 'tanh', 'identity')

**Usage in Benchmarks:**
- Provides neural network-based evaluation for regression tasks
- Alternative to KNN-based regression evaluation
- Automatic hyperparameter tuning for optimal performance

### 5. LocalOutlierFactorEvaluator (`outlier.py`)

Implements Local Outlier Factor algorithm for outlier detection using embeddings.

**Key Features:**
- Density-based outlier detection
- Configurable neighborhood size
- Multiple distance metrics
- AUROC scoring for outlier detection performance

**Parameters:**
- `model_params`: Dictionary containing LOF parameters
  - `n_neighbors`: Number of neighbors for density estimation
  - `metric`: Distance metric for neighbor search

**Usage in Benchmarks:**
- Primary evaluator for ADBench outlier detection benchmark
- Assesses how well embeddings preserve outlier patterns

### 6. IsolationForestEvaluator (`outlier.py`)

Implements Isolation Forest algorithm for outlier detection using embeddings.

**Key Features:**
- Tree-based outlier detection
- Configurable ensemble size
- Fast training and prediction
- AUROC scoring for outlier detection performance

**Parameters:**
- `model_params`: Dictionary containing Isolation Forest parameters
  - `n_estimators`: Number of trees in the ensemble

**Usage in Benchmarks:**
- Alternative outlier detection method in benchmarks
- Provides comparison with LOF-based evaluation

### 7. DeepSVDDEvaluator (`outlier.py`)

Implements Deep Support Vector Data Description algorithm for outlier detection using neural networks.

**Key Features:**
- Neural network-based outlier detection
- Deep learning approach to anomaly detection
- Dynamic hidden layer size computation based on input dimensionality
- Configurable architecture and training parameters
- PyOD-based implementation

**Parameters:**
- `dynamic_hidden_neurons`: Whether to automatically compute hidden layer sizes (default: False)
- `use_ae`: Whether to use autoencoder for pretraining (default: False)
- `hidden_neurons`: List of hidden layer sizes (default: [64, 32])
- `hidden_activation`: Activation function for hidden layers (default: "relu")
- `output_activation`: Activation function for output layer (default: "sigmoid")
- `optimizer`: Optimizer to use (default: "adam")
- `epochs`: Number of training epochs (default: 250)
- `batch_size`: Training batch size (default: 32)
- `dropout_rate`: Dropout rate for regularization (default: 0.2)
- `l2_regularizer`: L2 regularization strength (default: 0.1)
- `validation_size`: Proportion of data for validation (default: 0.1)
- `preprocessing`: Whether to apply preprocessing (default: True)
- `contamination`: Expected proportion of outliers (default: 0.1)
- `random_seed`: Random seed for reproducibility (default: 42)

**Dynamic Hidden Neurons:**
When `dynamic_hidden_neurons=True`, the evaluator automatically computes hidden layer sizes based on the number of input features, creating a decreasing sequence from approximately half the feature count down to 32.

**Usage in Benchmarks:**
- Neural network-based alternative to traditional outlier detection
- Particularly effective for complex, high-dimensional data
- Can capture non-linear patterns in embeddings

## Integration with Benchmarking Framework

### Workflow Integration
1. **Embedding Generation**: Embedding models generate vector representations
2. **Evaluator Selection**: Appropriate evaluators are chosen based on task type
3. **Training/Prediction**: Evaluators use embeddings to make predictions
4. **Performance Measurement**: Metrics are computed and collected
5. **State Reset**: Evaluators are reset between different datasets

### Parameter Sweeps
The benchmarking framework automatically creates evaluator instances with different parameter combinations:
- **Neighbor counts**: Typically 5-50 neighbors for KNN-based evaluators
- **Distance metrics**: Euclidean and cosine distance
- **Weighting schemes**: Uniform and distance-based weighting
- **Ensemble sizes**: Various tree counts for Isolation Forest

### Performance Metrics
- **Classification**: ROC-AUC score
- **Regression**: Mean Squared Error (MSE)
- **Outlier Detection**: Area Under ROC Curve (AUROC)

## Usage Examples

### Basic Usage
```python
from tabembedbench.evaluators import KNNClassifierEvaluator

# Create evaluator
evaluator = KNNClassifierEvaluator(
    num_neighbors=5,
    weights="uniform", 
    metric="euclidean"
)

# Use in benchmark
predictions = evaluator.get_prediction(embeddings, y_train, train=True)
test_predictions = evaluator.get_prediction(test_embeddings, train=False)
```

### Integration with Benchmarks
```python
from tabembedbench.benchmark import run_benchmark
from tabembedbench.evaluators import KNNClassifierEvaluator, LocalOutlierFactorEvaluator

evaluators = [
    KNNClassifierEvaluator(num_neighbors=5, metric="euclidean"),
    LocalOutlierFactorEvaluator(model_params={"n_neighbors": 10})
]

results = run_benchmark(
    embedding_models=embedding_models,
    evaluator_algorithms=evaluators
)
```

## Adding New Evaluators

To add a new evaluation algorithm:

1. **Inherit from AbstractEvaluator**:
   ```python
   class MyEvaluator(AbstractEvaluator):
       def __init__(self, ...):
           super().__init__(name="MyEvaluator", task_type="classification")
   ```

2. **Implement required methods**:
   - `get_prediction()`: Core prediction logic
   - `reset_evaluator()`: State cleanup
   - `get_parameters()`: Parameter reporting

3. **Follow naming conventions**: Use descriptive names that indicate the algorithm and task type

4. **Test integration**: Ensure compatibility with the benchmarking framework

## Dependencies

The evaluators module relies on:
- **scikit-learn**: For KNN, LOF, and Isolation Forest implementations
- **numpy**: For numerical operations and array handling
- **abc**: For abstract base class functionality

## Notes

- All evaluators are designed to work with numpy arrays of embeddings
- State management is crucial - always call `reset_evaluator()` between datasets
- Parameter dictionaries allow flexible configuration of underlying algorithms
- The module supports both supervised (classification/regression) and unsupervised (outlier detection) evaluation tasks
